{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FC_GANS_FINAL_PROJECT_413.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMUdOQSjLEqQWHcw/4BnJXo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangvi7/CSC413_FINAL_PROJECT/blob/main/FC_GANS_FINAL_PROJECT_413.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gUGxdNU166Q"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.nn import functional as F\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0enPTCgTSAU"
      },
      "source": [
        "device = torch.device('cuda:' + str(0) if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "SEED = 1\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83QT5x1m12Ii"
      },
      "source": [
        "!wget https://www.di.ens.fr/willow/research/seeing3Dchairs/data/rendered_chairs.tar\n",
        "# %mkdir -p chair_data\n",
        "!tar -xvf  'rendered_chairs.tar'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpAmP3uCMzwA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "ef197156-0987-4d21-a2d9-f807c8254f24"
      },
      "source": [
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision as tv\n",
        "import torch.utils.data as data\n",
        "\n",
        "# ---------------- DATASET LOADER -----------------\n",
        "\n",
        "def get_subset(indices, start, end):\n",
        "    return indices[start : start + end]\n",
        "\n",
        "# transforming from RBG to grayscale\n",
        "trainTransform  = tv.transforms.Compose([tv.transforms.Grayscale(num_output_channels=1),\n",
        "                                    tv.transforms.ToTensor()])\n",
        "\n",
        "dataset = torchvision.datasets.ImageFolder(\"rendered_chairs\", transform=trainTransform)\n",
        "\n",
        "# ------------ SPLIT SET INTO TRAIN, VALIDATION, TEST SETS -------------\n",
        "\n",
        "TRAIN_PCT, VALIDATION_PCT = 0.6, 0.2  # rest will go for test - TODO remove if we dont need validation set\n",
        "train_count = int(len(dataset) * TRAIN_PCT)\n",
        "validation_count = int(len(dataset) * VALIDATION_PCT)\n",
        "\n",
        "print(len(dataset))\n",
        "print(train_count)\n",
        "print(validation_count)\n",
        "\n",
        "\n",
        "indices = torch.randperm(len(dataset))\n",
        "\n",
        "# get subset of indices for train, valid, and test sets\n",
        "train_indices = get_subset(indices, 0, train_count)\n",
        "validation_indices = get_subset(indices, train_count, validation_count)\n",
        "test_indices = get_subset(indices, train_count + validation_count, len(dataset))\n",
        "\n",
        "# ---------------- CREATE DATA LOADER -----------------\n",
        "\n",
        "dataloaders = {\n",
        "    \"train\": torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=BATCH_SIZE, sampler=SubsetRandomSampler(train_indices)\n",
        "    ),\n",
        "    \"validation\": torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=BATCH_SIZE, sampler=SubsetRandomSampler(validation_indices)\n",
        "    ),\n",
        "    \"test\": torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=BATCH_SIZE, sampler=SubsetRandomSampler(test_indices)\n",
        "    ),\n",
        "}\n",
        "\n",
        "# ---------------- VISUALIZE IMAGE -----------------\n",
        "train_loader = dataloaders[\"train\"]\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy()\n",
        "# print(len(images)) # batch_size\n",
        "\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True  # Note: this is required for image truncation error.\n",
        "\n",
        "# for batch_i, (real_images, _) in enumerate(train_loader):\n",
        "#   print(batch_i, real_images)\n",
        "\n",
        "img = np.squeeze(images[0]).T # Tranpose for rbg to be shape (600, 600, 3) instead of (3, 600, 600).\n",
        "print(img.T.shape)\n",
        "\n",
        "# need to remove first dimension since currently shape is (3, 600, 600), but pyplot.imshow can only plot images of dimension (N,M) (grayscale) or (N,M,3)\n",
        "# img = img[0,:,:] (uncomment for grayscale)\n",
        "# print(img.shape)\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "86366\n",
            "51819\n",
            "17273\n",
            "(600, 600)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWvNvpYlcFzr"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, input_size, hidden_dim, output_size):\n",
        "    super(Discriminator, self).__init__()\n",
        "    \n",
        "    self.fc1 = nn.Linear(input_size, hidden_dim*4)\n",
        "    self.fc2 = nn.Linear(hidden_dim*4, hidden_dim*2)\n",
        "    self.fc3 = nn.Linear(hidden_dim*2, hidden_dim)\n",
        "    self.fc4 = nn.Linear(hidden_dim, output_size)\n",
        "    \n",
        "    # dropout layer \n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # flatten image to vector\n",
        "    x = x.view(-1, 600*600)\n",
        "    # use leaky relu for vanishing gradient\n",
        "    x = self.fc1(x)\n",
        "    x = F.leaky_relu(x, 0.2) # (input, negative_slope=0.2)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.leaky_relu(x, 0.2)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc3(x)\n",
        "    x = F.leaky_relu(x, 0.2)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc4(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYIMRyTme0HE"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, input_size, hidden_dim, output_size):\n",
        "    super(Generator, self).__init__()\n",
        "    \n",
        "    self.fc1 = nn.Linear(input_size, hidden_dim)\n",
        "    self.fc2 = nn.Linear(hidden_dim, hidden_dim*2)\n",
        "    self.fc3 = nn.Linear(hidden_dim*2, hidden_dim*4)\n",
        "    self.fc4 = nn.Linear(hidden_dim*4, output_size)\n",
        "\n",
        "    # dropout layer \n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = F.leaky_relu(x, 0.2)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.leaky_relu(x, 0.2)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc3(x)\n",
        "    x = F.leaky_relu(x, 0.2)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc4(x)\n",
        "    x = F.tanh(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OqyoSCqiudU"
      },
      "source": [
        "# hyper-params\n",
        "params = {\n",
        "    # Discriminator hyperparameters\n",
        "    \"input_size\": 360000,    # 600 x 600 image\n",
        "    \"d_output_size\": 1, \n",
        "    \"d_hidden_size\": 32,\n",
        "    # Generator hyperparams\n",
        "    \"z_size\": 256,       # latent vector\n",
        "    \"g_output_size\": 360000,\n",
        "    \"g_hidden_size\": 32,\n",
        "}\n",
        "# training params\n",
        "config = {\n",
        "    \"epoch\": 20,\n",
        "    \"lr\": 1e-3,\n",
        "    \"batch\": 64,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kV-_2r7GjDGe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4897ada-2ed8-4cef-fe94-308c98828e31"
      },
      "source": [
        "# instantiate discriminator and generator\n",
        "D = Discriminator(params[\"input_size\"], params[\"d_hidden_size\"], params[\"d_output_size\"])\n",
        "G = Generator(params[\"z_size\"], params[\"g_hidden_size\"], params[\"g_output_size\"])\n",
        "print(D)\n",
        "print(G)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discriminator(\n",
            "  (fc1): Linear(in_features=360000, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            ")\n",
            "Generator(\n",
            "  (fc1): Linear(in_features=256, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (fc4): Linear(in_features=128, out_features=360000, bias=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGjsCx2kkTUP"
      },
      "source": [
        "# discriminator loss: d_loss = d_real_loss + d_fake_loss\n",
        "def real_loss(D_out):\n",
        "  labels = torch.ones(D_out.size(0)).to(device) # real labels = 1\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  return criterion(D_out.squeeze(), labels)\n",
        "\n",
        "def fake_loss(D_out):\n",
        "  labels = torch.zeros(D_out.size(0)).to(device) # fake labels = 0\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  return criterion(D_out.squeeze(), labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXBmjzjimjMM"
      },
      "source": [
        "import torch.optim as optim\n",
        "d_optimizer = optim.Adam(D.parameters(), config[\"lr\"])\n",
        "g_optimizer = optim.Adam(G.parameters(), config[\"lr\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w50xKtO6nRtC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5875e4f3-12ce-4e7f-c5c1-b6c5551080ad"
      },
      "source": [
        "import pickle as pkl\n",
        "\n",
        "# training hyperparams\n",
        "num_epochs = config[\"epoch\"]\n",
        "batch_size = config[\"batch\"]\n",
        "z_size = params[\"z_size\"]\n",
        "\n",
        "# keep track of loss and generated, \"fake\" samples\n",
        "samples = []\n",
        "losses = []\n",
        "\n",
        "print_every = 400\n",
        "\n",
        "# Get some fixed data for sampling. These are images that are held\n",
        "# constant throughout training, and allow us to inspect the model's performance\n",
        "sample_size=16\n",
        "fixed_z = np.random.uniform(-1, 1, size=(sample_size, params[\"z_size\"]))\n",
        "fixed_z = torch.from_numpy(fixed_z).float().to(device)\n",
        "\n",
        "G = G.to(device)\n",
        "D = D.to(device)\n",
        "\n",
        "# train the network\n",
        "D.train()\n",
        "G.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    for batch_i, (real_images, _) in enumerate(train_loader):\n",
        "                \n",
        "        batch_size = real_images.size(0)\n",
        "        \n",
        "        ## Important rescaling step ## \n",
        "        real_images = real_images*2 - 1  # rescale input images from [0,1) to [-1, 1)\n",
        "        \n",
        "        # ============================================\n",
        "        #            TRAIN THE DISCRIMINATOR\n",
        "        # ============================================\n",
        "        \n",
        "        d_optimizer.zero_grad()\n",
        "        \n",
        "        # 1. Train with real images\n",
        "\n",
        "        # Compute the discriminator losses on real images \n",
        "        # smooth the real labels\n",
        "        real_images = real_images.to(device)\n",
        "        D_real = D(real_images)\n",
        "        d_real_loss = real_loss(D_real)\n",
        "        \n",
        "        # 2. Train with fake images\n",
        "        \n",
        "        # Generate fake images\n",
        "        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
        "        z = torch.from_numpy(z).float().to(device)\n",
        "        fake_images = G(z)\n",
        "                \n",
        "        # Compute the discriminator losses on fake images        \n",
        "        D_fake = D(fake_images)\n",
        "        d_fake_loss = fake_loss(D_fake)\n",
        "        \n",
        "        # add up loss and perform backprop\n",
        "        d_loss = d_real_loss + d_fake_loss\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "        \n",
        "        \n",
        "        # =========================================\n",
        "        #            TRAIN THE GENERATOR\n",
        "        # =========================================\n",
        "        g_optimizer.zero_grad()\n",
        "        \n",
        "        # 1. Train with fake images and flipped labels\n",
        "        \n",
        "        # Generate fake images\n",
        "        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
        "        z = torch.from_numpy(z).float().to(device)\n",
        "        fake_images = G(z)\n",
        "        \n",
        "        # Compute the discriminator losses on fake images \n",
        "        # using flipped labels!\n",
        "        D_fake = D(fake_images)\n",
        "        g_loss = real_loss(D_fake) # use real loss to flip labels\n",
        "        \n",
        "        # perform backprop\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        # Print some loss stats\n",
        "        if batch_i % print_every == 0:\n",
        "            # print discriminator and generator loss\n",
        "            print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
        "                    epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n",
        "\n",
        "    \n",
        "    ## AFTER EACH EPOCH##\n",
        "    # append discriminator loss and generator loss\n",
        "    losses.append((d_loss.item(), g_loss.item()))\n",
        "    \n",
        "    # generate and save sample, fake images\n",
        "    G.eval() # eval mode for generating samples\n",
        "    samples_z = G(fixed_z)\n",
        "    samples.append(samples_z)\n",
        "    G.train() # back to train mode\n",
        "\n",
        "\n",
        "# Save training generator samples\n",
        "with open('train_samples.pkl', 'wb') as f:\n",
        "    pkl.dump(samples, f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [    1/   20] | d_loss: 1.3984 | g_loss: 1.2093\n",
            "Epoch [    1/   20] | d_loss: 0.9940 | g_loss: 0.5753\n",
            "Epoch [    1/   20] | d_loss: 1.0961 | g_loss: 0.6488\n",
            "Epoch [    2/   20] | d_loss: 0.9238 | g_loss: 1.0998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1naVv_bxnZXK"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "losses = np.array(losses)\n",
        "plt.plot(losses.T[0], label='Discriminator')\n",
        "plt.plot(losses.T[1], label='Generator')\n",
        "plt.title(\"Training Losses\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHimnL7jiofb"
      },
      "source": [
        "# helper function for viewing a list of passed in sample images\n",
        "def view_samples(epoch, samples):\n",
        "    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)\n",
        "    for ax, img in zip(axes.flatten(), samples[epoch]):\n",
        "        img = img.detach()\n",
        "        ax.xaxis.set_visible(False)\n",
        "        ax.yaxis.set_visible(False)\n",
        "        im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n",
        "\n",
        "# Load samples from generator, taken while training\n",
        "with open('train_samples.pkl', 'rb') as f:\n",
        "    samples = pkl.load(f)\n",
        "\n",
        "# -1 indicates final epoch's samples (the last in the list)\n",
        "view_samples(-1, samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMHiZqMSIS0S"
      },
      "source": [
        "# try subsample, crop out image whitespace, dcgans if training too slow."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}